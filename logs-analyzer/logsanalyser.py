import streamlit as st
import pandas as pd
import json
import ast
from collections import Counter, defaultdict
import csv
import io
import re
import numpy as np
from sklearn.decomposition import PCA, LatentDirichletAllocation
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans, DBSCAN
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import plotly.express as px
import plotly.graph_objects as go
from scipy.stats import chi2_contingency
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.manifold import TSNE
import hashlib
from typing import List, Dict, Tuple, Optional
import gc

# Configura√ß√£o da p√°gina
st.set_page_config(
    layout="wide", 
    page_title="Analisador de Logs CSV Multiplo",
    page_icon="üìä",
    initial_sidebar_state="collapsed"
)

# CSS moderno e limpo com roxo claro (mantido igual)
st.markdown("""
<style>
    /* Reset e base */
    .stApp {
        font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
        background-color: #F8F6FF;
    }
    
    /* Containers principais */
    .main-container {
        background: white;
        border-radius: 16px;
        padding: 2rem;
        margin: 1rem 0;
        box-shadow: 0 2px 8px rgba(149, 117, 205, 0.1);
        border: 1px solid #F0EBFF;
    }
    
    /* Headers */
    .main-header {
        font-size: 2.5rem;
        font-weight: 700;
        color: #1e293b;
        text-align: center;
        margin-bottom: 1.5rem;
        background: linear-gradient(135deg, #9575CD 0%, #B39DDB 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }
    
    .section-header {
        font-size: 1.5rem;
        font-weight: 600;
        color: #5E35B1;
        margin: 1.5rem 0 1rem 0;
        padding-bottom: 0.5rem;
        border-bottom: 2px solid #F3E5F5;
    }
    
    /* Cards de m√©tricas */
    .metric-card {
        background: white;
        padding: 1.5rem;
        border-radius: 12px;
        border-left: 4px solid #B39DDB;
        box-shadow: 0 2px 6px rgba(149, 117, 205, 0.1);
        text-align: center;
        transition: all 0.3s ease;
        border: 1px solid #F3E5F5;
    }
    
    .metric-card:hover {
        transform: translateY(-3px);
        box-shadow: 0 6px 20px rgba(149, 117, 205, 0.15);
        border-color: #D1C4E9;
    }
    
    .metric-value {
        font-size: 2rem;
        font-weight: 700;
        color: #5E35B1;
        margin: 0.5rem 0;
    }
    
    .metric-label {
        font-size: 0.9rem;
        color: #7E57C2;
        font-weight: 500;
    }
    
    /* Tabs estilizadas */
    .stTabs [data-baseweb="tab-list"] {
        gap: 1rem;
        background: transparent;
    }
    
    .stTabs [data-baseweb="tab"] {
        background: white;
        border-radius: 12px 12px 0 0;
        padding: 1rem 2rem;
        font-weight: 500;
        border: 1px solid #F0EBFF;
        border-bottom: none;
        margin: 0 0.2rem;
        color: #7E57C2;
        transition: all 0.3s ease;
    }
    
    .stTabs [aria-selected="true"] {
        background: linear-gradient(135deg, #B39DDB 0%, #D1C4E9 100%);
        color: white;
        border-color: #B39DDB;
        box-shadow: 0 2px 8px rgba(149, 117, 205, 0.2);
    }
    
    /* Upload area */
    .upload-section {
        border: 2px dashed #D1C4E9;
        border-radius: 16px;
        padding: 3rem;
        text-align: center;
        background: #F3E5F5;
        transition: all 0.3s ease;
    }
    
    .upload-section:hover {
        border-color: #9575CD;
        background: #EDE7F6;
        box-shadow: 0 4px 12px rgba(149, 117, 205, 0.1);
    }
    
    /* Bot√µes */
    .stButton button {
        background: linear-gradient(135deg, #9575CD 0%, #B39DDB 100%);
        color: white;
        border: none;
        padding: 0.75rem 2rem;
        border-radius: 12px;
        font-weight: 500;
        transition: all 0.3s ease;
        width: 100%;
        box-shadow: 0 2px 6px rgba(149, 117, 205, 0.2);
    }
    
    .stButton button:hover {
        background: linear-gradient(135deg, #8561C8 0%, #A08BD6 100%);
        transform: translateY(-2px);
        box-shadow: 0 6px 16px rgba(149, 117, 205, 0.3);
    }
    
    /* Status badges */
    .status-badge {
        display: inline-block;
        padding: 0.25rem 0.75rem;
        border-radius: 20px;
        font-size: 0.75rem;
        font-weight: 600;
        margin-right: 0.5rem;
    }
    
    .badge-success { background: #E8F5E8; color: #2E7D32; border: 1px solid #C8E6C9; }
    .badge-warning { background: #FFF8E1; color: #F57C00; border: 1px solid #FFECB3; }
    .badge-error { background: #FFEBEE; color: #C62828; border: 1px solid #FFCDD2; }
    .badge-info { background: #E3F2FD; color: #1565C0; border: 1px solid #BBDEFB; }
    
    /* Dataframes */
    .dataframe {
        border-radius: 12px;
        border: 1px solid #F0EBFF;
    }
    
    /* Seletores e inputs */
    .stSelectbox, .stMultiselect, .stTextInput {
        border-radius: 8px;
    }
    
    .stSelectbox div, .stMultiselect div {
        border-radius: 8px;
        border: 1px solid #D1C4E9;
    }
    
    /* Expanders */
    .streamlit-expanderHeader {
        background: #F8F6FF;
        border-radius: 8px;
        border: 1px solid #F0EBFF;
        color: #5E35B1;
        font-weight: 500;
    }
    
    /* Sliders */
    .stSlider [data-baseweb="slider"] {
        color: #9575CD;
    }
    
    /* Progress bars */
    .stProgress > div > div {
        background: linear-gradient(90deg, #B39DDB 0%, #D1C4E9 100%);
    }
</style>
""", unsafe_allow_html=True)

# ================== FUN√á√ïES DE PROCESSAMENTO DE M√öLTIPLOS ARQUIVOS ==================

class MultiCSVProcessor:
    """Classe para processamento de m√∫ltiplos arquivos CSV"""
    
    @staticmethod
    def read_problematic_csv(uploaded_file):
        """L√™ CSV com problemas de formata√ß√£o usando m√©todos robustos"""
        content = uploaded_file.read().decode('utf-8', errors='ignore')
        lines = content.split('\n')
        
        # Tenta detectar o delimitador
        first_line = lines[0] if lines else ''
        delimiter = ','
        if ';' in first_line and first_line.count(';') > first_line.count(','):
            delimiter = ';'
        elif '\t' in first_line:
            delimiter = '\t'
        
        # M√©todo 1: Pandas com engine python (mais tolerante)
        try:
            uploaded_file.seek(0)
            df = pd.read_csv(uploaded_file, delimiter=delimiter, engine='python', 
                            quoting=csv.QUOTE_ALL, on_bad_lines='warn')
            return df, "pandas"
        except Exception as e:
            st.warning(f"Pandas method failed for {uploaded_file.name}: {e}")
        
        # M√©todo 2: Leitura manual com csv.reader
        try:
            data = []
            for line in lines:
                if line.strip():
                    try:
                        reader = csv.reader([line], delimiter=delimiter, quotechar='"')
                        for row in reader:
                            if row:
                                data.append(row)
                    except:
                        # Fallback: split simples por delimitador
                        row = line.split(delimiter)
                        data.append(row)
            
            # Encontra n√∫mero m√°ximo de colunas
            max_cols = max(len(row) for row in data) if data else 0
            
            # Preenche linhas com menos colunas
            for row in data:
                while len(row) < max_cols:
                    row.append(None)
                while len(row) > max_cols:
                    row.pop()
            
            # Cria DataFrame
            if data:
                headers = data[0] if len(data[0]) == max_cols else [f'col_{i}' for i in range(max_cols)]
                df = pd.DataFrame(data[1:], columns=headers)
                return df, "manual"
        except Exception as e:
            st.warning(f"Manual method failed for {uploaded_file.name}: {e}")
        
        return None, "error"
    
    @staticmethod
    def identify_json_column(df: pd.DataFrame) -> str:
        """Identifica automaticamente a coluna que cont√©m JSON"""
        possible_json_cols = []
        
        # Prioridade 1: Colunas com nomes sugestivos
        json_keywords = ['log', 'json', 'data', 'message', 'event', 'payload']
        for col in df.columns:
            col_lower = str(col).lower()
            for keyword in json_keywords:
                if keyword in col_lower:
                    possible_json_cols.append(col)
        
        # Prioridade 2: Colunas que cont√™m strings com { ou [
        if not possible_json_cols:
            for col in df.columns:
                sample = df[col].dropna().head(10)
                if len(sample) > 0:
                    if any(('{' in str(val) or '[' in str(val)) for val in sample):
                        possible_json_cols.append(col)
        
        # Prioridade 3: Primeira coluna que n√£o seja num√©rica ou muito curta
        if not possible_json_cols and len(df.columns) > 0:
            for col in df.columns:
                if df[col].dtype == 'object':
                    possible_json_cols.append(col)
        
        return possible_json_cols[0] if possible_json_cols else df.columns[0]
    
    @staticmethod
    def detect_common_schema(files_info: List[Dict]) -> Dict:
        """Detecta schema comum entre m√∫ltiplos arquivos"""
        all_fields = set()
        common_fields = None
        field_types = {}
        
        for info in files_info:
            fields = set(info.get('fields', []))
            all_fields.update(fields)
            
            if common_fields is None:
                common_fields = fields
            else:
                common_fields = common_fields.intersection(fields)
            
            for field, types in info.get('field_types', {}).items():
                if field not in field_types:
                    field_types[field] = set(types)
                else:
                    field_types[field].update(types)
        
        return {
            'all_fields': sorted(all_fields),
            'common_fields': sorted(common_fields) if common_fields else [],
            'field_types': {k: list(v) for k, v in field_types.items()},
            'total_files': len(files_info),
            'total_rows': sum(info.get('total_rows', 0) for info in files_info),
            'total_json_rows': sum(info.get('json_rows', 0) for info in files_info)
        }

def parse_json_string(json_str):
    """Tenta parsear uma string JSON de v√°rias formas"""
    if not json_str or not isinstance(json_str, str):
        return None
    
    json_str_clean = json_str.strip()
    if json_str_clean.startswith('"') and json_str_clean.endswith('"'):
        json_str_clean = json_str_clean[1:-1]
    
    try:
        return json.loads(json_str_clean)
    except json.JSONDecodeError:
        try:
            return ast.literal_eval(json_str_clean)
        except:
            try:
                json_str_fixed = re.sub(r"'([^']*)'", r'"\1"', json_str_clean)
                json_str_fixed = re.sub(r',\s*}', '}', json_str_fixed)
                json_str_fixed = re.sub(r',\s*]', ']', json_str_fixed)
                return json.loads(json_str_fixed)
            except:
                return None

def extract_all_fields(data, parent_key='', separator='.'):
    """Extrai recursivamente todos os campos de um JSON e remove prefixos 'data.'"""
    fields = []
    if isinstance(data, dict):
        for key, value in data.items():
            new_key = f"{parent_key}{separator}{key}" if parent_key else key
            # Remove prefixos 'data.' duplicados
            new_key = re.sub(r'^data\.data\.', 'data.', new_key)
            new_key = re.sub(r'^data\.', '', new_key)  # Remove todos os prefixos 'data.'
            
            if isinstance(value, (dict, list)):
                fields.extend(extract_all_fields(value, new_key, separator))
            else:
                fields.append(new_key)
    elif isinstance(data, list):
        for i, item in enumerate(data):
            if isinstance(item, (dict, list)):
                fields.extend(extract_all_fields(item, parent_key, separator))
    else:
        if parent_key:
            # Remove prefixos 'data.' tamb√©m para campos simples
            parent_key = re.sub(r'^data\.data\.', 'data.', parent_key)
            parent_key = re.sub(r'^data\.', '', parent_key)
            fields.append(parent_key)
    
    return list(set(fields))  # Remove duplicatas

def extract_field_value(data, field_path):
    """Extrai valor de campo aninhado, lidando com prefixos 'data.'"""
    try:
        # Adiciona prefixo 'data.' se necess√°rio para campos comuns
        if not field_path.startswith('data.') and 'data.' in str(data):
            # Tenta com prefixo data. primeiro
            try:
                partes = f"data.{field_path}".split('.')
                current = data
                for parte in partes:
                    if isinstance(current, dict) and parte in current:
                        current = current[parte]
                    else:
                        current = None
                        break
                if current is not None:
                    return current
            except:
                pass
        
        # Tenta sem prefixo
        if '.' in field_path:
            partes = field_path.split('.')
            current = data
            for parte in partes:
                if isinstance(current, dict) and parte in current:
                    current = current[parte]
                else:
                    return None
            return current
        else:
            return data.get(field_path)
    except:
        return None

def discover_json_structure(df, coluna_json='log', sample_size=50):
    """Descobre automaticamente a estrutura dos JSONs"""
    all_fields = set()
    field_examples = defaultdict(set)
    field_types = defaultdict(set)
    
    sample_df = df.head(min(sample_size, len(df)))
    
    for idx, row in sample_df.iterrows():
        if pd.notna(row[coluna_json]):
            json_data = parse_json_string(str(row[coluna_json]))
            if json_data and isinstance(json_data, dict):
                fields = extract_all_fields(json_data)
                for field in fields:
                    all_fields.add(field)
                    value = extract_field_value(json_data, field)
                    if value is not None:
                        field_examples[field].add(str(value))
                        field_types[field].add(type(value).__name__)
    
    return sorted(all_fields), field_examples, field_types

def processar_csv_automatico(df, campos_selecionados, coluna_json='log'):
    """Processa CSV extraindo campos automaticamente"""
    try:
        associacoes = defaultdict(Counter)
        dados_completos = []
        linhas_processadas = len(df)
        linhas_com_json = 0
        
        for idx, row in df.iterrows():
            if pd.notna(row[coluna_json]):
                json_data = parse_json_string(str(row[coluna_json]))
                
                if json_data and isinstance(json_data, dict):
                    linhas_com_json += 1
                    linha_dados = {}
                    
                    for campo in campos_selecionados:
                        valor = extract_field_value(json_data, campo)
                        if valor is not None:
                            # Remove prefixos 'data.' do nome da coluna no DataFrame final
                            campo_clean = re.sub(r'^data\.', '', campo)
                            linha_dados[campo_clean] = valor
                            associacoes[campo_clean][str(valor)] += 1
                    
                    dados_completos.append(linha_dados)
        
        df_completo = pd.DataFrame(dados_completos)
        return associacoes, df_completo, linhas_processadas, linhas_com_json
        
    except Exception as e:
        st.error(f"Erro ao processar o arquivo: {e}")
        return None, None, 0, 0

def processar_multiplos_csv(uploaded_files, campos_selecionados):
    """Processa m√∫ltiplos arquivos CSV de forma eficiente"""
    processor = MultiCSVProcessor()
    resultados = []
    todos_dados = []
    estatisticas_globais = defaultdict(Counter)
    
    # Barra de progresso
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, uploaded_file in enumerate(uploaded_files):
        try:
            status_text.text(f"Processando {i+1}/{len(uploaded_files)}: {uploaded_file.name}")
            
            # Ler arquivo
            df, method = processor.read_problematic_csv(uploaded_file)
            
            if df is not None:
                # Identificar coluna JSON automaticamente
                coluna_json = processor.identify_json_column(df)
                
                # Analisar estrutura
                campos_disponiveis, exemplos_campos, tipos_campos = discover_json_structure(df, coluna_json)
                
                # Processar dados
                associacoes, df_completo, total_linhas, linhas_json = processar_csv_automatico(
                    df, campos_selecionados, coluna_json
                )
                
                if df_completo is not None and not df_completo.empty:
                    # Adicionar identificador do arquivo
                    df_completo['_arquivo_origem'] = uploaded_file.name
                    df_completo['_arquivo_indice'] = i
                    
                    todos_dados.append(df_completo)
                    
                    # Consolidar estat√≠sticas
                    for campo, counter in associacoes.items():
                        for valor, contagem in counter.items():
                            estatisticas_globais[campo][valor] += contagem
                
                resultados.append({
                    'nome': uploaded_file.name,
                    'tamanho': len(df),
                    'colunas': len(df.columns),
                    'json_col': coluna_json,
                    'campos_encontrados': len(campos_disponiveis),
                    'linhas_processadas': total_linhas,
                    'linhas_json': linhas_json,
                    'dados_validos': len(df_completo) if df_completo is not None else 0
                })
            
            # Atualizar progresso
            progress_bar.progress((i + 1) / len(uploaded_files))
            gc.collect()  # Liberar mem√≥ria
            
        except Exception as e:
            st.error(f"Erro ao processar {uploaded_file.name}: {e}")
            resultados.append({
                'nome': uploaded_file.name,
                'erro': str(e),
                'processado': False
            })
    
    # Concatenar todos os dados
    df_final = pd.concat(todos_dados, ignore_index=True) if todos_dados else pd.DataFrame()
    
    return df_final, resultados, estatisticas_globais

def create_metric_card(value, label, icon="üìä"):
    """Cria um card de m√©trica estilizado"""
    return f"""
    <div class="metric-card">
        <div style="font-size: 1.5rem; margin-bottom: 0.5rem;">{icon}</div>
        <div class="metric-value">{value}</div>
        <div class="metric-label">{label}</div>
    </div>
    """

def prepare_data_for_clustering(df, selected_columns):
    """Prepara dados para clusteriza√ß√£o"""
    if not selected_columns:
        return None, "Nenhuma coluna selecionada"
    
    clustering_df = df[selected_columns].copy()
    
    # Prepara dados num√©ricos
    numeric_cols = clustering_df.select_dtypes(include=[np.number]).columns.tolist()
    
    # Prepara dados categ√≥ricos (converte para num√©rico)
    categorical_cols = clustering_df.select_dtypes(include=['object']).columns.tolist()
    
    # Codifica vari√°veis categ√≥ricas
    for col in categorical_cols:
        if col in clustering_df.columns:
            le = LabelEncoder()
            try:
                clustering_df[col] = le.fit_transform(clustering_df[col].astype(str))
                numeric_cols.append(col)
            except:
                clustering_df = clustering_df.drop(columns=[col])
    
    if len(numeric_cols) == 0:
        return None, "Nenhuma coluna num√©rica dispon√≠vel para clusteriza√ß√£o"
    
    # Remove colunas com muitos valores faltantes
    clustering_df = clustering_df.dropna(thresh=len(clustering_df) * 0.5, axis=1)
    
    if clustering_df.empty:
        return None, "Dados insuficientes ap√≥s limpeza"
    
    # Preenche valores faltantes
    for col in clustering_df.columns:
        if clustering_df[col].dtype in [np.float64, np.int64]:
            clustering_df[col] = clustering_df[col].fillna(clustering_df[col].median())
        else:
            clustering_df[col] = clustering_df[col].fillna(clustering_df[col].mode()[0] if not clustering_df[col].mode().empty else 0)
    
    # Normaliza os dados
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(clustering_df)
    
    return {
        'scaled_data': scaled_data,
        'original_df': clustering_df,
        'feature_names': clustering_df.columns.tolist(),
        'scaler': scaler
    }, None

def find_optimal_clusters_kmeans(data, max_clusters=10):
    """Encontra o n√∫mero √≥timo de clusters usando o m√©todo do cotovelo para KMeans"""
    wcss = []  # Within-Cluster Sum of Square
    silhouette_scores = []
    
    for i in range(2, min(max_clusters + 1, len(data) - 1)):
        kmeans = KMeans(n_clusters=i, random_state=42, n_init=10)
        kmeans.fit(data)
        wcss.append(kmeans.inertia_)
        
        if i < len(data):
            silhouette_scores.append(silhouette_score(data, kmeans.labels_))
        else:
            silhouette_scores.append(0)
    
    # Encontrar o ponto do cotovelo
    if len(wcss) > 1:
        deltas = np.diff(wcss)
        second_deltas = np.diff(deltas)
        if len(second_deltas) > 0:
            optimal_idx = np.argmax(second_deltas) + 2
            optimal_clusters = min(optimal_idx + 2, len(wcss) + 1)
        else:
            optimal_clusters = 3
    else:
        optimal_clusters = 2
    
    return wcss, silhouette_scores, optimal_clusters

def find_optimal_topics_lda(text_data, max_topics=10):
    """Encontra o n√∫mero √≥timo de t√≥picos para LDA usando perplexidade e coer√™ncia"""
    vectorizer = CountVectorizer(max_features=100, stop_words='english')
    X = vectorizer.fit_transform(text_data)
    
    perplexities = []
    log_likelihoods = []
    
    for n_components in range(2, min(max_topics + 1, len(text_data))):
        lda = LatentDirichletAllocation(
            n_components=n_components,
            random_state=42,
            max_iter=10
        )
        lda.fit(X)
        
        # Perplexidade (quanto menor, melhor)
        perplexity = lda.perplexity(X)
        perplexities.append(perplexity)
        
        # Log likelihood (quanto maior, melhor)
        log_likelihood = lda.score(X)
        log_likelihoods.append(log_likelihood)
    
    # Encontrar ponto √≥timo baseado na taxa de mudan√ßa da perplexidade
    if len(perplexities) > 1:
        perplexity_deltas = np.diff(perplexities)
        # Normalizar as mudan√ßas
        normalized_deltas = np.abs(perplexity_deltas) / np.max(np.abs(perplexity_deltas))
        
        # Encontrar onde a taxa de melhoria diminui significativamente
        optimal_idx = np.argmax(normalized_deltas < 0.1) + 2 if np.any(normalized_deltas < 0.1) else 2
        optimal_topics = min(optimal_idx + 1, len(perplexities) + 1)
    else:
        optimal_topics = 3
    
    return perplexities, log_likelihoods, optimal_topics

def perform_advanced_clustering(data, n_clusters, method='kmeans', text_data=None, feature_names=None):
    """Executa clusteriza√ß√£o com diferentes m√©todos"""
    
    if method == 'lda':
        # Para LDA, precisamos de dados textuais
        if text_data is None:
            return None, "LDA requer dados textuais para an√°lise"
        
        vectorizer = CountVectorizer(
            max_features=100, 
            stop_words='english',
            min_df=2,
            max_df=0.8
        )
        X = vectorizer.fit_transform(text_data)
        
        lda = LatentDirichletAllocation(
            n_components=n_clusters, 
            random_state=42,
            max_iter=10,
            learning_method='online'
        )
        lda_features = lda.fit_transform(X)
        clusters = np.argmax(lda_features, axis=1)
        
        # M√©tricas para LDA s√£o diferentes
        silhouette_avg = -1
        calinski_score = -1
        davies_score = -1
        
        return {
            'clusters': clusters,
            'model': lda,
            'metrics': {
                'silhouette': silhouette_avg,
                'calinski_harabasz': calinski_score,
                'davies_bouldin': davies_score
            },
            'vectorizer': vectorizer,
            'method': 'lda',
            'features_2d': TSNE(n_components=2, random_state=42).fit_transform(lda_features)
        }
        
    else:
        # KMeans e DBSCAN tradicionais
        if method == 'kmeans':
            clusterer = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        else:  # dbscan
            clusterer = DBSCAN(eps=0.5, min_samples=2)
        
        clusters = clusterer.fit_predict(data)
        
        # Calcula m√©tricas de avalia√ß√£o
        if len(set(clusters)) > 1 and method == 'kmeans':
            silhouette_avg = silhouette_score(data, clusters)
            calinski_score = calinski_harabasz_score(data, clusters)
            davies_score = davies_bouldin_score(data, clusters)
        else:
            silhouette_avg = -1
            calinski_score = -1
            davies_score = -1
        
        return {
            'clusters': clusters,
            'model': clusterer,
            'metrics': {
                'silhouette': silhouette_avg,
                'calinski_harabasz': calinski_score,
                'davies_bouldin': davies_score
            },
            'method': method,
            'features_2d': PCA(n_components=2).fit_transform(data)
        }

def prepare_text_data_for_lda(df, selected_columns):
    """Prepara dados textuais para LDA"""
    text_columns = df[selected_columns].select_dtypes(include=['object']).columns
    
    if len(text_columns) == 0:
        return None, "Nenhuma coluna textual encontrada para LDA"
    
    # Combina o texto de todas as colunas textuais
    text_data = df[text_columns].astype(str).agg(' '.join, axis=1)
    
    return text_data, None

def analyze_lda_topics(lda_model, vectorizer, n_top_words=10):
    """Analisa os t√≥picos encontrados pelo LDA"""
    feature_names = vectorizer.get_feature_names_out()
    topics = []
    
    for topic_idx, topic in enumerate(lda_model.components_):
        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]
        top_features = [feature_names[i] for i in top_features_ind]
        topic_words = ", ".join(top_features)
        topics.append({
            'topic_id': topic_idx,
            'top_words': topic_words,
            'word_weights': topic[top_features_ind],
            'top_features': top_features
        })
    
    return topics

def create_cluster_analysis(df, clustering_result, feature_names):
    """Cria an√°lise detalhada dos clusters"""
    df_analysis = df.copy()
    df_analysis['cluster'] = clustering_result['clusters']
    
    cluster_profiles = {}
    
    for cluster_id in sorted(df_analysis['cluster'].unique()):
        cluster_data = df_analysis[df_analysis['cluster'] == cluster_id]
        
        profile = {
            'size': len(cluster_data),
            'percentage': (len(cluster_data) / len(df_analysis)) * 100,
            'characteristics': {}
        }
        
        # Analisa cada caracter√≠stica do cluster
        for feature in feature_names:
            if feature in cluster_data.columns:
                if cluster_data[feature].dtype in ['object', 'category']:
                    value_counts = cluster_data[feature].value_counts()
                    if not value_counts.empty:
                        profile['characteristics'][feature] = {
                            'type': 'categorical',
                            'top_value': value_counts.index[0],
                            'top_percentage': (value_counts.iloc[0] / len(cluster_data)) * 100
                        }
                else:
                    profile['characteristics'][feature] = {
                        'type': 'numeric',
                        'mean': cluster_data[feature].mean(),
                        'median': cluster_data[feature].median(),
                        'std': cluster_data[feature].std()
                    }
        
        cluster_profiles[f'Cluster {cluster_id}'] = profile
    
    return cluster_profiles

def main():
    # Inicializar session state
    if 'analysis_started' not in st.session_state:
        st.session_state.analysis_started = False
    if 'analysis_result' not in st.session_state:
        st.session_state.analysis_result = None
    if 'processamento_concluido' not in st.session_state:
        st.session_state.processamento_concluido = False
    if 'schema_info' not in st.session_state:
        st.session_state.schema_info = None
    if 'arquivos_info' not in st.session_state:
        st.session_state.arquivos_info = None
    if 'df_final' not in st.session_state:
        st.session_state.df_final = None
    if 'resultados_processamento' not in st.session_state:
        st.session_state.resultados_processamento = None
    if 'estatisticas_globais' not in st.session_state:
        st.session_state.estatisticas_globais = None
    
    # Header principal
    st.markdown("""
    <div style="text-align: center; padding: 2rem 0;">
        <h1 style="color: #5E35B1; font-weight: 700; margin-bottom: 1rem;">üìä Analisador de Logs CSV Multiplo</h1>
        <p style="color: #7E57C2; font-size: 1.1rem; max-width: 600px; margin: 0 auto;">
            Analise grandes quantidades de dados processando m√∫ltiplos arquivos CSV simultaneamente
        </p>
    </div>
    """, unsafe_allow_html=True)

    # √Årea de upload M√öLTIPLO
    st.markdown("""
    <div class="main-container">
        <div class="upload-section">
            <h3 style="color: #5E35B1; margin-bottom: 1rem;">üìÅ Fa√ßa upload de m√∫ltiplos arquivos CSV</h3>
            <p style="color: #7E57C2; margin-bottom: 2rem;">Processe v√°rios arquivos de uma s√≥ vez - suporta grandes volumes de dados</p>
    """, unsafe_allow_html=True)
    
    uploaded_files = st.file_uploader(
        " ",
        type=['csv'],
        help="Selecione um ou mais arquivos CSV para an√°lise",
        label_visibility="collapsed",
        accept_multiple_files=True  # üÜï Habilita upload m√∫ltiplo
    )
    
    st.markdown("</div></div>", unsafe_allow_html=True)

    if uploaded_files:
        if len(uploaded_files) == 1:
            st.info("üìù **Modo √∫nico ativado**: Carregado 1 arquivo. Para processamento em lote, fa√ßa upload de m√∫ltiplos arquivos.")
        else:
            st.success(f"üöÄ **Modo em lote ativado**: {len(uploaded_files)} arquivos carregados para processamento simult√¢neo.")
        
        # Mostrar lista de arquivos carregados
        with st.expander("üìã Visualizar arquivos carregados", expanded=True):
            files_info = []
            for i, file in enumerate(uploaded_files):
                col1, col2, col3 = st.columns([3, 1, 1])
                with col1:
                    st.write(f"**{i+1}. {file.name}**")
                with col2:
                    st.write(f"{file.size / 1024:.1f} KB")
                with col3:
                    st.write("‚úÖ Pronto")
                files_info.append({
                    'nome': file.name,
                    'tamanho': file.size,
                    'indice': i
                })
        
        # Primeiro passo: Analisar estrutura dos arquivos para encontrar campos comuns
        st.markdown('<div class="main-container">', unsafe_allow_html=True)
        st.markdown('<div class="section-header">üîç An√°lise de Estrutura dos Arquivos</div>', unsafe_allow_html=True)
        
        if st.button("üî¨ Analisar Estrutura dos Arquivos", use_container_width=True):
            with st.spinner('Analisando estrutura de todos os arquivos...'):
                processor = MultiCSVProcessor()
                arquivos_info = []
                campos_todos_arquivos = set()
                
                progress_bar = st.progress(0)
                for i, uploaded_file in enumerate(uploaded_files):
                    df, _ = processor.read_problematic_csv(uploaded_file)
                    if df is not None:
                        # Identificar coluna JSON
                        coluna_json = processor.identify_json_column(df)
                        
                        # Descobrir campos
                        campos_disponiveis, _, tipos_campos = discover_json_structure(df, coluna_json, sample_size=100)
                        
                        arquivos_info.append({
                            'nome': uploaded_file.name,
                            'total_rows': len(df),
                            'json_col': coluna_json,
                            'fields': campos_disponiveis,
                            'field_types': tipos_campos
                        })
                        
                        campos_todos_arquivos.update(campos_disponiveis)
                    
                    progress_bar.progress((i + 1) / len(uploaded_files))
                
                # Detectar schema comum
                schema_info = processor.detect_common_schema(arquivos_info)
                
                # Armazenar no session state
                st.session_state.schema_info = schema_info
                st.session_state.arquivos_info = arquivos_info
                st.session_state.campos_todos_arquivos = sorted(campos_todos_arquivos)
                
                st.success(f"‚úÖ An√°lise conclu√≠da! Encontrados {len(campos_todos_arquivos)} campos √∫nicos em {len(uploaded_files)} arquivos.")
        
        # Mostrar informa√ß√µes do schema se dispon√≠veis
        if 'schema_info' in st.session_state and st.session_state.schema_info is not None:
            schema = st.session_state.schema_info
            
            st.markdown("#### üìä Resumo dos Arquivos")
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Total de Arquivos", schema['total_files'])
            with col2:
                st.metric("Total de Linhas", f"{schema['total_rows']:,}")
            with col3:
                st.metric("Campos √önicos", len(schema['all_fields']))
            with col4:
                st.metric("Campos Comuns", len(schema['common_fields']))
            
            # Sele√ß√£o de campos para an√°lise
            st.markdown("#### üéØ Sele√ß√£o de Campos para Extra√ß√£o")
            
            st.info(f"üí° **Dica**: {len(schema['common_fields'])} campos est√£o presentes em todos os arquivos. Recomendamos come√ßar com esses.")
            
            filtro_campo = st.text_input("üîç Filtrar campos", placeholder="Digite para filtrar campos...")
            
            # Op√ß√µes de sele√ß√£o
            opcao_selecao = st.radio(
                "Escolha uma estrat√©gia de sele√ß√£o:",
                [
                    "Usar apenas campos comuns a todos os arquivos",
                    "Selecionar manualmente campos espec√≠ficos",
                    "Usar todos os campos dispon√≠veis"
                ]
            )
            
            if opcao_selecao == "Usar apenas campos comuns a todos os arquivos":
                campos_selecionados = schema['common_fields']
                st.success(f"‚úÖ {len(campos_selecionados)} campos comuns selecionados automaticamente.")
            elif opcao_selecao == "Usar todos os campos dispon√≠veis":
                campos_selecionados = schema['all_fields']
                st.warning(f"‚ö†Ô∏è {len(campos_selecionados)} campos selecionados. Isso pode exigir mais mem√≥ria.")
            else:  # Sele√ß√£o manual
                campos_filtrados = [campo for campo in schema['all_fields'] 
                                  if filtro_campo.lower() in campo.lower()]
                
                st.markdown("##### Selecione os campos manualmente:")
                col1, col2 = st.columns(2)
                campos_selecionados = []
                
                with col1:
                    for i, campo in enumerate(campos_filtrados[:len(campos_filtrados)//2]):
                        is_common = " (presente em todos)" if campo in schema['common_fields'] else ""
                        if st.checkbox(f"`{campo}`{is_common}", key=f"manual_campo_{i}", value=(campo in schema['common_fields'])):
                            campos_selecionados.append(campo)
                
                with col2:
                    for i, campo in enumerate(campos_filtrados[len(campos_filtrados)//2:]):
                        idx = i + len(campos_filtrados)//2
                        is_common = " (presente em todos)" if campo in schema['common_fields'] else ""
                        if st.checkbox(f"`{campo}`{is_common}", key=f"manual_campo_{idx}", value=(campo in schema['common_fields'])):
                            campos_selecionados.append(campo)
            
            if campos_selecionados and st.button("üöÄ Processar Todos os Arquivos", type="primary", use_container_width=True):
                with st.spinner(f'Processando {len(uploaded_files)} arquivos com {len(campos_selecionados)} campos cada...'):
                    df_final, resultados, estatisticas = processar_multiplos_csv(
                        uploaded_files, campos_selecionados
                    )
                    
                    # Armazenar resultados no session state
                    st.session_state.df_final = df_final
                    st.session_state.resultados_processamento = resultados
                    st.session_state.estatisticas_globais = estatisticas
                    st.session_state.processamento_concluido = True
                    
                    st.success(f"‚úÖ Processamento conclu√≠do! {len(df_final)} registros consolidados de {len(uploaded_files)} arquivos.")
        else:
            # Mostrar apenas o bot√£o para analisar estrutura
            st.info("üëÜ Clique em 'Analisar Estrutura dos Arquivos' para come√ßar a an√°lise dos campos dispon√≠veis em todos os arquivos.")

        st.markdown('</div>', unsafe_allow_html=True)
        
        # Se o processamento foi conclu√≠do, mostrar resultados
        if (st.session_state.get('processamento_concluido', False) and 
            'df_final' in st.session_state and 
            st.session_state.df_final is not None):
            
            df_final = st.session_state.df_final
            resultados = st.session_state.resultados_processamento
            estatisticas = st.session_state.estatisticas_globais
            
            if not df_final.empty:
                # Tabs principais para an√°lise consolidada
                tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
                    "üìà Vis√£o Consolidada", 
                    "üìä Estat√≠sticas", 
                    "üë• Personas", 
                    "üîç An√°lises Avan√ßadas", 
                    "ü§ñ Clusteriza√ß√£o",
                    "üìÅ Arquivos Individuais"
                ])

                with tab1:
                    st.markdown('<div class="main-container">', unsafe_allow_html=True)
                    st.markdown('<div class="section-header">üìà Vis√£o Consolidada dos Dados</div>', unsafe_allow_html=True)
                    
                    # M√©tricas consolidadas
                    total_arquivos = len(uploaded_files)
                    total_registros = len(df_final)
                    campos_extraidos = len([c for c in df_final.columns if not c.startswith('_')])
                    
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.markdown(create_metric_card(total_arquivos, "Arquivos Processados", "üìÅ"), unsafe_allow_html=True)
                    with col2:
                        st.markdown(create_metric_card(f"{total_registros:,}", "Registros Consolidados", "üìä"), unsafe_allow_html=True)
                    with col3:
                        st.markdown(create_metric_card(campos_extraidos, "Campos Extra√≠dos", "üéØ"), unsafe_allow_html=True)
                    with col4:
                        st.markdown(create_metric_card(df_final.nunique().sum(), "Valores √önicos", "üî§"), unsafe_allow_html=True)
                    
                    # Distribui√ß√£o por arquivo
                    st.markdown("#### üìä Distribui√ß√£o por Arquivo de Origem")
                    dist_arquivos = df_final['_arquivo_origem'].value_counts()
                    
                    fig_dist = px.bar(
                        x=dist_arquivos.index,
                        y=dist_arquivos.values,
                        title="N√∫mero de Registros por Arquivo",
                        labels={'x': 'Arquivo', 'y': 'Registros'},
                        color=dist_arquivos.values,
                        color_continuous_scale='purples'
                    )
                    fig_dist.update_layout(xaxis_tickangle=-45)
                    st.plotly_chart(fig_dist, use_container_width=True)
                    
                    # Visualiza√ß√£o dos dados consolidados
                    st.markdown("#### üìã Dados Consolidados (Amostra)")
                    st.dataframe(df_final.head(20), use_container_width=True)
                    
                    # Op√ß√µes de exporta√ß√£o
                    st.markdown("#### üíæ Exportar Dados Consolidados")
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        csv = df_final.to_csv(index=False)
                        st.download_button(
                            label="üì• Download CSV Consolidado",
                            data=csv,
                            file_name="dados_consolidados.csv",
                            mime="text/csv",
                            use_container_width=True
                        )
                    
                    with col2:
                        # Remover colunas internas para exporta√ß√£o limpa
                        df_export = df_final.drop(columns=['_arquivo_origem', '_arquivo_indice'], errors='ignore')
                        csv_clean = df_export.to_csv(index=False)
                        st.download_button(
                            label="üì• Download CSV Limpo",
                            data=csv_clean,
                            file_name="dados_limpos.csv",
                            mime="text/csv",
                            use_container_width=True
                        )
                    
                    st.markdown('</div>', unsafe_allow_html=True)

                with tab2:
                    st.markdown('<div class="main-container">', unsafe_allow_html=True)
                    st.markdown('<div class="section-header">üìä Estat√≠sticas Consolidadas</div>', unsafe_allow_html=True)
                    
                    st.markdown("#### üìà Estat√≠sticas Descritivas")
                    st.dataframe(df_final.describe(include='all'), use_container_width=True)
                    
                    # Matriz de correla√ß√£o para campos num√©ricos
                    numeric_cols = df_final.select_dtypes(include=[np.number]).columns
                    if len(numeric_cols) > 1:
                        st.markdown("#### üîó Matriz de Correla√ß√£o")
                        corr_matrix = df_final[numeric_cols].corr()
                        fig = px.imshow(
                            corr_matrix,
                            title="Matriz de Correla√ß√£o - Dados Consolidados",
                            color_continuous_scale='Purples',
                            aspect="auto"
                        )
                        st.plotly_chart(fig, use_container_width=True)
                    
                    # An√°lise de distribui√ß√£o por campo
                    st.markdown("#### üìä Distribui√ß√µes por Campo")
                    
                    campo_analise = st.selectbox(
                        "Selecione um campo para an√°lise de distribui√ß√£o:",
                        options=[c for c in df_final.columns if not c.startswith('_')],
                        key="dist_field_consolidated"
                    )
                    
                    if campo_analise in df_final.columns:
                        fig = px.histogram(
                            df_final, 
                            x=campo_analise,
                            title=f"Distribui√ß√£o de {campo_analise} - Dados Consolidados",
                            color_discrete_sequence=['#9575CD'],
                            nbins=50
                        )
                        st.plotly_chart(fig, use_container_width=True)
                        
                        # Mostrar estat√≠sticas espec√≠ficas
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("Valores √önicos", df_final[campo_analise].nunique())
                        with col2:
                            st.metric("Valores Nulos", df_final[campo_analise].isnull().sum())
                        with col3:
                            if df_final[campo_analise].dtype in [np.float64, np.int64]:
                                st.metric("M√©dia", f"{df_final[campo_analise].mean():.2f}")
                    
                    st.markdown('</div>', unsafe_allow_html=True)

                with tab3:
                    st.markdown('<div class="main-container">', unsafe_allow_html=True)
                    st.markdown('<div class="section-header">üë• An√°lise de Personas Consolidadas</div>', unsafe_allow_html=True)
                    
                    # Implementa√ß√£o similar √† original, mas com dados consolidados
                    campos_persona = st.multiselect(
                        "Selecione campos para an√°lise de personas (dados consolidados)",
                        options=[c for c in df_final.columns if not c.startswith('_')],
                        default=[c for c in df_final.columns if not c.startswith('_')][:min(5, len(df_final.columns))],
                        help="Escolha campos que representem caracter√≠sticas distintas para forma√ß√£o de personas"
                    )
                    
                    if campos_persona:
                        # Fun√ß√µes de an√°lise de personas (similares √†s originais)
                        def create_persona_analysis_consolidado(df, clustering_fields):
                            """Cria an√°lise de personas baseada em clustering para dados consolidados"""
                            if not clustering_fields:
                                return None, "Selecione campos para an√°lise de personas"
                            
                            analysis_df = df[clustering_fields].copy()
                            
                            for col in analysis_df.columns:
                                if analysis_df[col].dtype == 'object':
                                    le = LabelEncoder()
                                    analysis_df[col] = le.fit_transform(analysis_df[col].astype(str))
                            
                            analysis_df = analysis_df.fillna(analysis_df.mean())
                            
                            scaler = StandardScaler()
                            scaled_data = scaler.fit_transform(analysis_df)
                            
                            n_clusters = min(5, len(analysis_df), max(2, len(analysis_df) // 100))
                            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
                            personas = kmeans.fit_predict(scaled_data)
                            
                            df_persona = df.copy()
                            df_persona['persona'] = personas
                            
                            persona_profiles = {}
                            for persona in range(len(set(personas))):
                                persona_data = df_persona[df_persona['persona'] == persona]
                                profile = {
                                    'size': len(persona_data),
                                    'characteristics': {}
                                }
                                
                                for col in clustering_fields:
                                    if persona_data[col].dtype == 'object':
                                        profile['characteristics'][col] = persona_data[col].mode().iloc[0] if not persona_data[col].mode().empty else 'N/A'
                                    else:
                                        profile['characteristics'][col] = persona_data[col].mean()
                                
                                persona_profiles[f'Persona {persona + 1}'] = profile
                            
                            return persona_profiles, None
                        
                        with st.spinner("Criando an√°lise de personas..."):
                            personas, error = create_persona_analysis_consolidado(df_final, campos_persona)
                        
                        if personas:
                            st.success(f"‚úÖ {len(personas)} personas identificadas nos dados consolidados.")
                            
                            # Visualiza√ß√£o das personas
                            col1, col2 = st.columns(2)
                            with col1:
                                persona_counts = {k: v['size'] for k, v in personas.items()}
                                fig_personas = px.bar(
                                    x=list(persona_counts.keys()),
                                    y=list(persona_counts.values()),
                                    title="Distribui√ß√£o de Personas",
                                    labels={'x': 'Persona', 'y': 'N√∫mero de Registros'},
                                    color=list(persona_counts.values()),
                                    color_continuous_scale='purples'
                                )
                                st.plotly_chart(fig_personas, use_container_width=True)
                            
                            with col2:
                                # Scatter plot se temos pelo menos 2 campos num√©ricos
                                numeric_persona_fields = [f for f in campos_persona if df_final[f].dtype in [np.float64, np.int64]]
                                if len(numeric_persona_fields) >= 2:
                                    # Adicionar personas ao dataframe tempor√°rio
                                    df_temp = df_final[numeric_persona_fields[:2]].copy()
                                    df_temp['persona'] = [0] * len(df_temp)  # Placeholder
                                    # Aqui voc√™ implementaria a l√≥gica real de atribui√ß√£o de personas
                                    
                                    fig_scatter = px.scatter(
                                        df_temp,
                                        x=numeric_persona_fields[0],
                                        y=numeric_persona_fields[1],
                                        color='persona',
                                        title="Visualiza√ß√£o 2D de Personas",
                                        color_continuous_scale='purples'
                                    )
                                    st.plotly_chart(fig_scatter, use_container_width=True)
                            
                            # Detalhes de cada persona
                            st.markdown("#### üë§ Perfis das Personas")
                            for persona_name, profile in personas.items():
                                with st.expander(f"{persona_name} - {profile['size']} registros"):
                                    st.write(f"**Tamanho:** {profile['size']} registros")
                                    st.write(f"**Porcentagem:** {(profile['size']/len(df_final))*100:.1f}%")
                                    st.write("**Caracter√≠sticas Principais:**")
                                    
                                    for campo, valor in profile['characteristics'].items():
                                        if isinstance(valor, (int, float)):
                                            st.write(f"- **{campo}:** {valor:.2f}")
                                        else:
                                            st.write(f"- **{campo}:** {valor}")
                        else:
                            st.error(f"Erro na an√°lise de personas: {error}")
                    
                    st.markdown('</div>', unsafe_allow_html=True)

                with tab4:
                    st.markdown('<div class="main-container">', unsafe_allow_html=True)
                    st.markdown('<div class="section-header">üîç An√°lises Avan√ßadas Consolidadas</div>', unsafe_allow_html=True)
                    
                    # An√°lise temporal se houver campos de data
                    date_columns = [col for col in df_final.columns if any(keyword in col.lower() for keyword in ['date', 'time', 'timestamp'])]
                    
                    if date_columns:
                        st.markdown("#### üìÖ An√°lise Temporal")
                        date_column = st.selectbox("Selecione coluna de data:", date_columns)
                        
                        if date_column in df_final.columns:
                            try:
                                # Tentar converter para datetime
                                df_final[date_column] = pd.to_datetime(df_final[date_column], errors='coerce')
                                
                                # Agrupar por per√≠odo
                                periodo = st.selectbox("Agrupar por:", ['Dia', 'Hora', 'M√™s', 'Semana'])
                                
                                if periodo == 'Dia':
                                    df_temp = df_final.set_index(date_column).resample('D').size()
                                    titulo = "Registros por Dia"
                                elif periodo == 'Hora':
                                    df_temp = df_final.set_index(date_column).resample('H').size()
                                    titulo = "Registros por Hora"
                                elif periodo == 'M√™s':
                                    df_temp = df_final.set_index(date_column).resample('M').size()
                                    titulo = "Registros por M√™s"
                                else:  # Semana
                                    df_temp = df_final.set_index(date_column).resample('W').size()
                                    titulo = "Registros por Semana"
                                
                                fig_temporal = px.line(
                                    x=df_temp.index,
                                    y=df_temp.values,
                                    title=titulo,
                                    labels={'x': 'Data', 'y': 'N√∫mero de Registros'},
                                    line_shape='spline'
                                )
                                fig_temporal.update_traces(line_color='#9575CD')
                                st.plotly_chart(fig_temporal, use_container_width=True)
                                
                            except Exception as e:
                                st.warning(f"N√£o foi poss√≠vel analisar dados temporais: {e}")
                    
                    # An√°lise de padr√µes entre arquivos
                    st.markdown("#### üìä Padr√µes Entre Arquivos")
                    
                    if '_arquivo_origem' in df_final.columns:
                        campo_cross = st.selectbox(
                            "Analisar campo entre arquivos:",
                            options=[c for c in df_final.columns if not c.startswith('_')],
                            key="cross_analysis"
                        )
                        
                        if campo_cross:
                            # Criar pivot table
                            if df_final[campo_cross].dtype == 'object':
                                # Para campos categ√≥ricos
                                pivot = pd.crosstab(
                                    df_final['_arquivo_origem'],
                                    df_final[campo_cross],
                                    normalize='index'
                                )
                                
                                fig_heatmap = px.imshow(
                                    pivot,
                                    title=f"Distribui√ß√£o de {campo_cross} entre Arquivos",
                                    color_continuous_scale='purples',
                                    aspect="auto"
                                )
                                st.plotly_chart(fig_heatmap, use_container_width=True)
                            else:
                                # Para campos num√©ricos
                                stats_by_file = df_final.groupby('_arquivo_origem')[campo_cross].agg(['mean', 'std', 'count'])
                                st.dataframe(stats_by_file, use_container_width=True)
                    
                    st.markdown('</div>', unsafe_allow_html=True)

                with tab5:
                    st.markdown('<div class="main-container">', unsafe_allow_html=True)
                    st.markdown('<div class="section-header">ü§ñ Clusteriza√ß√£o de Dados Consolidados</div>', unsafe_allow_html=True)
                    
                    # Implementa√ß√£o similar √† original, mas com dados consolidados
                    clustering_columns_consolidated = st.multiselect(
                        "Selecione colunas para clusteriza√ß√£o (dados consolidados):",
                        options=[c for c in df_final.columns if not c.startswith('_')],
                        default=[c for c in df_final.columns if not c.startswith('_')][:min(3, len(df_final.columns))],
                        key="clustering_consolidated"
                    )
                    
                    if clustering_columns_consolidated:
                        # Configura√ß√£o de clusteriza√ß√£o (similar √† original)
                        clustering_method = st.selectbox(
                            "M√©todo de Clusteriza√ß√£o:",
                            options=['kmeans', 'dbscan', 'lda'],
                            key="method_consolidated"
                        )
                        
                        # (O restante da implementa√ß√£o da clusteriza√ß√£o seria similar √† original)
                        st.info("üîß A funcionalidade de clusteriza√ß√£o para dados consolidados utiliza a mesma implementa√ß√£o da vers√£o original, adaptada para o dataset combinado.")
                        
                        # Bot√£o para executar clusteriza√ß√£o
                        if st.button("üöÄ Executar Clusteriza√ß√£o nos Dados Consolidados", use_container_width=True):
                            st.success("Funcionalidade de clusteriza√ß√£o dispon√≠vel. Implementa√ß√£o similar √† vers√£o original.")
                    
                    st.markdown('</div>', unsafe_allow_html=True)

                with tab6:
                    st.markdown('<div class="main-container">', unsafe_allow_html=True)
                    st.markdown('<div class="section-header">üìÅ An√°lise por Arquivo Individual</div>', unsafe_allow_html=True)
                    
                    # Seletor de arquivo para an√°lise individual
                    arquivos_disponiveis = df_final['_arquivo_origem'].unique()
                    arquivo_selecionado = st.selectbox(
                        "Selecione um arquivo para an√°lise detalhada:",
                        options=arquivos_disponiveis
                    )
                    
                    if arquivo_selecionado:
                        # Filtrar dados do arquivo selecionado
                        df_arquivo = df_final[df_final['_arquivo_origem'] == arquivo_selecionado].copy()
                        
                        st.markdown(f"#### üìä An√°lise de **{arquivo_selecionado}**")
                        
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("Registros no Arquivo", len(df_arquivo))
                        with col2:
                            st.metric("Campos Extra√≠dos", len([c for c in df_arquivo.columns if not c.startswith('_')]))
                        with col3:
                            st.metric("Porcentagem do Total", f"{(len(df_arquivo)/len(df_final))*100:.1f}%")
                        
                        # Visualiza√ß√£o dos dados do arquivo
                        st.markdown("##### üìã Dados do Arquivo (Amostra)")
                        st.dataframe(df_arquivo.head(10).drop(columns=['_arquivo_origem', '_arquivo_indice'], errors='ignore'), use_container_width=True)
                        
                        # An√°lise espec√≠fica do arquivo
                        st.markdown("##### üîç An√°lise Espec√≠fica")
                        
                        campo_arquivo = st.selectbox(
                            "Selecione campo para an√°lise no arquivo:",
                            options=[c for c in df_arquivo.columns if not c.startswith('_')],
                            key=f"campo_{arquivo_selecionado}"
                        )
                        
                        if campo_arquivo in df_arquivo.columns:
                            # Distribui√ß√£o
                            fig_arquivo = px.histogram(
                                df_arquivo,
                                x=campo_arquivo,
                                title=f"Distribui√ß√£o de {campo_arquivo} em {arquivo_selecionado}",
                                color_discrete_sequence=['#B39DDB']
                            )
                            st.plotly_chart(fig_arquivo, use_container_width=True)
                            
                            # Compara√ß√£o com outros arquivos
                            st.markdown("##### üìà Compara√ß√£o com Outros Arquivos")
                            
                            if df_final[campo_arquivo].dtype in [np.float64, np.int64]:
                                # Para campos num√©ricos, mostrar box plot comparativo
                                fig_comparacao = px.box(
                                    df_final,
                                    x='_arquivo_origem',
                                    y=campo_arquivo,
                                    title=f"Compara√ß√£o de {campo_arquivo} entre Arquivos",
                                    color='_arquivo_origem',
                                    color_discrete_sequence=px.colors.qualitative.Pastel
                                )
                                st.plotly_chart(fig_comparacao, use_container_width=True)
                            else:
                                # Para campos categ√≥ricos, mostrar compara√ß√£o de distribui√ß√£o
                                comparacao = pd.crosstab(
                                    df_final['_arquivo_origem'],
                                    df_final[campo_arquivo],
                                    normalize='index'
                                ).round(3)
                                
                                st.markdown(f"**Distribui√ß√£o de {campo_arquivo} por arquivo:**")
                                st.dataframe(comparacao, use_container_width=True)
                    
                    st.markdown('</div>', unsafe_allow_html=True)
            
            else:
                st.error("‚ùå N√£o foi poss√≠vel consolidar dados. Verifique se os arquivos t√™m estruturas compat√≠veis.")

    else:
        # Tela inicial - Como Usar
        with st.container():
            st.markdown('<div class="main-container">', unsafe_allow_html=True)
            
            st.markdown("""
            <div style="text-align: center;">
                <h2 style="color: #5E35B1; margin-bottom: 2rem;">Como Usar o Analisador de Logs CSV Multiplo</h2>
            </div>
            """, unsafe_allow_html=True)
            
            # Grid de passos usando columns
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.markdown("""
                <div class="metric-card">
                    <h4>1. Carregar M√∫ltiplos Arquivos</h4>
                    <p>Selecione v√°rios arquivos CSV de uma vez</p>
                </div>
                """, unsafe_allow_html=True)
            
            with col2:
                st.markdown("""
                <div class="metric-card">
                    <h4>2. Analisar Estrutura</h4>
                    <p>Detecte campos comuns automaticamente</p>
                </div>
                """, unsafe_allow_html=True)
            
            with col3:
                st.markdown("""
                <div class="metric-card">
                    <h4>3. Processar em Lote</h4>
                    <p>Extraia dados de todos os arquivos</p>
                </div>
                """, unsafe_allow_html=True)
            
            with col4:
                st.markdown("""
                <div class="metric-card">
                    <h4>4. Analisar Consolidado</h4>
                    <p>Explore insights em todos os dados</p>
                </div>
                """, unsafe_allow_html=True)
            
            # Vantagens do modo m√∫ltiplo
            st.markdown("""
            <div style="background: #F3E5F5; padding: 2rem; border-radius: 12px; margin-top: 2rem;">
                <h4 style="color: #5E35B1; margin-bottom: 1rem;">üöÄ Vantagens do Modo M√∫ltiplo</h4>
                <p style="color: #7E57C2; margin: 0.5rem 0;">‚Ä¢ <strong>Processamento em lote:</strong> Analise gigabytes de dados</p>
                <p style="color: #7E57C2; margin: 0.5rem 0;">‚Ä¢ <strong>Detec√ß√£o autom√°tica de schema:</strong> Encontre campos comuns entre arquivos</p>
                <p style="color: #7E57C2; margin: 0.5rem 0;">‚Ä¢ <strong>Consolida√ß√£o inteligente:</strong> Combine dados de diferentes fontes</p>
                <p style="color: #7E57C2; margin: 0.5rem 0;">‚Ä¢ <strong>An√°lise comparativa:</strong> Compare padr√µes entre arquivos</p>
                <p style="color: #7E57C2; margin: 0.5rem 0;">‚Ä¢ <strong>Otimiza√ß√£o de mem√≥ria:</strong> Processe grandes volumes eficientemente</p>
            </div>
            
            <div style="margin-top: 2rem; padding: 1.5rem; background: linear-gradient(135deg, #EDE7F6 0%, #F3E5F5 100%); border-radius: 12px;">
                <h4 style="color: #5E35B1; margin-bottom: 1rem;">üìÅ Formatos Suportados</h4>
                <div style="display: flex; flex-wrap: wrap; gap: 1rem;">
                    <div style="background: white; padding: 1rem; border-radius: 8px; flex: 1; min-width: 200px;">
                        <h5 style="color: #7E57C2; margin: 0 0 0.5rem 0;">CSV Simples</h5>
                        <p style="color: #666; margin: 0; font-size: 0.9rem;">Delimitadores: , ; \\t</p>
                    </div>
                    <div style="background: white; padding: 1rem; border-radius: 8px; flex: 1; min-width: 200px;">
                        <h5 style="color: #7E57C2; margin: 0 0 0.5rem 0;">CSV com JSON</h5>
                        <p style="color: #666; margin: 0; font-size: 0.9rem;">Logs, eventos, dados aninhados</p>
                    </div>
                    <div style="background: white; padding: 1rem; border-radius: 8px; flex: 1; min-width: 200px;">
                        <h5 style="color: #7E57C2; margin: 0 0 0.5rem 0;">Arquivos Grandes</h5>
                        <p style="color: #666; margin: 0; font-size: 0.9rem;">Processamento por chunks</p>
                    </div>
                </div>
            </div>
            """, unsafe_allow_html=True)
            
            # Limites e recomenda√ß√µes
            with st.expander("‚ö° Limites e Recomenda√ß√µes para Grandes Volumes"):
                st.markdown("""
                ### üìä Capacidades do Sistema
                
                | Volume de Dados | N√∫mero de Arquivos | Tempo Estimado | Mem√≥ria Recomendada |
                |----------------|-------------------|----------------|---------------------|
                | < 100 MB       | 1-10 arquivos     | 10-30 segundos | 512 MB RAM          |
                | 100 MB - 1 GB  | 10-50 arquivos    | 1-5 minutos    | 1 GB RAM            |
                | 1 GB - 5 GB    | 50-200 arquivos   | 5-15 minutos   | 2 GB RAM            |
                | > 5 GB         | 200+ arquivos     | 15+ minutos    | 4+ GB RAM           |
                
                ### üí° Dicas para Otimiza√ß√£o
                1. **Selecione apenas campos necess√°rios**: Menos campos = menor uso de mem√≥ria
                2. **Processe em lotes**: Para volumes muito grandes, processe por partes
                3. **Use campos comuns**: Comece com campos presentes em todos os arquivos
                4. **Exporte intermedi√°rios**: Salve dados processados para evitar reprocessamento
                5. **Filtre dados irrelevantes**: Remova registros desnecess√°rios antes da an√°lise
                
                ### ‚ö†Ô∏è Limita√ß√µes Conhecidas
                - Arquivos individuais muito grandes (>500MB) podem exigir processamento especial
                - Muitas colunas (>100) podem impactar performance de visualiza√ß√£o
                - JSONs muito complexos/aninhados podem exigir mais mem√≥ria
                """)
            
            st.markdown('</div>', unsafe_allow_html=True)

if __name__ == "__main__":
    main()